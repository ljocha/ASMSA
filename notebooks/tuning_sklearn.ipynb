{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn --user\n",
    "# !pip install bayesian-optimization --user\n",
    "# !pip install fast-ml --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "\n",
    "from keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "from src import asmsa\n",
    "from src.gan import GAN\n",
    "from src.visualizer import GAN_visualizer\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import nglview as nv\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf92f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scorer accuracy\n",
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input files\n",
    "%cd ~\n",
    "\n",
    "# input conformation\n",
    "#conf = \"alaninedipeptide_H.pdb\"\n",
    "conf = \"trpcage_correct.pdb\"\n",
    "\n",
    "# input trajectory\n",
    "# atom numbering must be consistent with {conf}\n",
    "\n",
    "#traj = \"alaninedipeptide_reduced.xtc\"\n",
    "traj = \"trpcage_red.xtc\"\n",
    "\n",
    "# input topology\n",
    "# expected to be produced with \n",
    "#    gmx pdb2gmx -f {conf} -p {topol} -n {index} \n",
    "\n",
    "# Gromacs changes atom numbering, the index file must be generated and used as well\n",
    "\n",
    "#topol = \"topol.top\"\n",
    "topol = \"topol_correct.top\"\n",
    "index = 'index_correct.ndx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0968f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = md.load(traj,top=conf)\n",
    "idx=tr[0].top.select(\"name CA\")\n",
    "#idx=tr[0].top.select(\"element != H\")\n",
    "tr.superpose(tr[0],atom_indices=idx)\n",
    "geom = np.moveaxis(tr.xyz ,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = nv.show_mdtraj(tr)\n",
    "v.clear()\n",
    "v.add_representation(\"licorice\")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f96b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sparse and dense feture extensions of IC\n",
    "density = 2 # integer in [1, n_atoms-1]\n",
    "sparse_dists = asmsa.NBDistancesSparse(geom.shape[0], density=density)\n",
    "dense_dists = asmsa.NBDistancesDense(geom.shape[0])\n",
    "\n",
    "# mol = asmsa.Molecule(conf,topol)\n",
    "# mol = asmsa.Molecule(conf,topol,fms=[sparse_dists])\n",
    "mol = asmsa.Molecule(pdb=conf,top=topol,ndx=index,fms=[sparse_dists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebbcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mol.intcoord(geom).T\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train, test = train_test_split(X_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9127ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(X_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f09b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowdim = 2\n",
    "prior = 'normal'\n",
    "molecule_shape = (X_train.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(params):\n",
    "    model = Sequential()\n",
    "    # input layer\n",
    "    model.add(Dense(params['neurons'], input_dim=np.prod(molecule_shape), activation=params['activation']))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    # hidden layers\n",
    "    model.add(Dense(params['neurons'], activation=params['activation']))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(params['neurons'], activation=params['activation']))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    #output layer\n",
    "    model.add(Dense(lowdim, activation='linear'))\n",
    "    mol = Input(shape=molecule_shape)\n",
    "    lowdim = model(mol)\n",
    "    return Model(mol, lowdim, name=\"Encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder tuning\n",
    "\n",
    "def build_decoder(params):\n",
    "    model = Sequential()\n",
    "    model._name = \"Decoder\"\n",
    "    # input layer\n",
    "    model.add(Dense(params['neurons'], input_dim=lowdim, activation='linear'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    # hidden layers\n",
    "    model.add(Dense(params['neurons'], activation=params['activation']))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(params['neurons'], activation=params['activation']))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    # output layer\n",
    "    model.add(Dense(np.prod(molecule_shape), activation=params['activation']))\n",
    "    model.add(Reshape(molecule_shape))\n",
    "    lowdim = Input(shape=(lowdim,))\n",
    "    mol = model(lowdim)\n",
    "    return Model(lowdim, mol, name=\"Decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43180cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator tuning\n",
    "\n",
    "def build_discriminator(params):\n",
    "    model = Sequential()\n",
    "    model._name = \"Discriminator\"\n",
    "    model.add(Flatten(input_shape=(lowdim,)))\n",
    "    model.add(Dense(params['neurons']))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(params['neurons']))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(params['neurons']))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(params['neurons']))\n",
    "\n",
    "    mol = Input(shape=(lowdim,))\n",
    "    validity = model(mol)\n",
    "    return Model(mol, validity, name=\"Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAEModel(Model):\n",
    "    def __init__(self,enc,dec,disc,lowdim,prior):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "        self.disc = disc\n",
    "        self.lowdim = lowdim\n",
    "        self.prior = prior\n",
    "\n",
    "\n",
    "    def compile(self,\n",
    "        opt = Adam(0.0002,0.5),\t# FIXME: justify\n",
    "        ae_loss_fn = MeanSquaredError(),\n",
    "# XXX: logits as in https://keras.io/guides/customizing_what_happens_in_fit/, \n",
    "# hope it works as the discriminator output is never used directly\n",
    "        disc_loss_fn = BinaryCrossentropy(from_logits=True)\t\n",
    "    ):\n",
    "\n",
    "        super().compile()\n",
    "        self.opt = opt\n",
    "        self.ae_loss_fn = ae_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "\n",
    "        \n",
    "    def train_step(self,batch):\n",
    "        def _get_prior(name, shape):\n",
    "            if name == \"normal\":\n",
    "                return tf.random.normal(shape=shape)\n",
    "            if name == \"uniform\":\n",
    "                return tf.random.uniform(shape=shape)\n",
    "            \n",
    "            raise ValueError(f\"Invalid prior type '{name}'. Choose from 'normal|uniform'\")\n",
    "        \n",
    "        if isinstance(batch,tuple):\n",
    "            batch = batch[0]\n",
    "\n",
    "        batch_size = tf.shape(batch)[0]\n",
    "\n",
    "# improve AE to reconstruct\n",
    "        with tf.GradientTape(persistent=True) as ae_tape:\n",
    "            reconstruct = self.dec(self.enc(batch))\n",
    "            ae_loss = self.ae_loss_fn(batch,reconstruct)\n",
    "\n",
    "        enc_grads = ae_tape.gradient(ae_loss, self.enc.trainable_weights)\n",
    "        self.opt.apply_gradients(zip(enc_grads,self.enc.trainable_weights))\n",
    "\n",
    "        dec_grads = ae_tape.gradient(ae_loss, self.dec.trainable_weights)\n",
    "        self.opt.apply_gradients(zip(dec_grads,self.dec.trainable_weights))\n",
    "\n",
    "# improve discriminator\n",
    "        rand_low = _get_prior(self.prior, (batch_size, self.lowdim))\n",
    "        better_low = self.enc(batch)\n",
    "        low = tf.concat([rand_low,better_low],axis=0)\n",
    "\n",
    "        labels = tf.concat([tf.ones((batch_size,1)), tf.zeros((batch_size,1))], axis=0)\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\t# guide\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            pred = self.disc(low)\n",
    "            disc_loss = self.disc_loss_fn(labels,pred)\n",
    "\n",
    "        disc_grads = disc_tape.gradient(disc_loss,self.disc.trainable_weights)\n",
    "        self.opt.apply_gradients(zip(disc_grads,self.disc.trainable_weights))\n",
    "\n",
    "# teach encoder to cheat\n",
    "        alltrue = tf.ones((batch_size,1))\n",
    "\n",
    "        with tf.GradientTape() as cheat_tape:\n",
    "            cheat = self.disc(self.enc(batch))\n",
    "            cheat_loss = self.disc_loss_fn(alltrue,cheat)\n",
    "\n",
    "        cheat_grads = cheat_tape.gradient(cheat_loss,self.enc.trainable_weights)\n",
    "        self.opt.apply_gradients(zip(cheat_grads,self.enc.trainable_weights))\n",
    "\n",
    "        return { 'ae_loss' : ae_loss, 'd_loss' : disc_loss }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6dcd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    params['neurons'] = round(neurons)\n",
    "    params['activation'] = activationL[round(activation)]\n",
    "    params['batch_size'] = round(batch_size)\n",
    "    params['epochs'] = round(epochs)\n",
    "    \n",
    "#     neurons = round(neurons)\n",
    "#     activation = activationL[round(activation)]\n",
    "#     batch_size = round(batch_size)\n",
    "#     epochs = round(epochs)\n",
    "    def nn_cl_fun():\n",
    "        opt = Adam(lr = learning_rate)\n",
    "        \n",
    "        enc = build_encoder(params)\n",
    "        dec = build_decoder(params)\n",
    "        disc = build_discriminator(params)\n",
    "        \n",
    "        aae = AAEModel(enc, dec, disc, lowdim, prior)\n",
    "        aae.compile(opt=opt) #also parametrizable ae_loss_fn and others\n",
    "#         nn = Sequential()\n",
    "#         nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
    "#         nn.add(Dense(neurons, activation=activation))\n",
    "#         nn.add(Dense(1, activation='sigmoid'))\n",
    "#         nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return aae\n",
    "    es = EarlyStopping(monitor='ae_loss', mode='min', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, train, validation, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07449f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paramaters\n",
    "params_nn ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(64, 500),\n",
    "    'epochs':(20, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nn_ = nn_bo.min['params']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
