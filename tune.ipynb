{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6851ea37",
   "metadata": {},
   "source": [
    "# ASMSA: Tune AAE model hyperparameters\n",
    "\n",
    "**Previous step**\n",
    "- [prepare.ipynb](prepare.ipynb): Download and sanity check input files\n",
    "\n",
    "**Next steps**\n",
    "- [train.ipynb](train.ipynb): Use results of previous tuning in more thorough training\n",
    "- [md.ipynb](md.ipynb): Use a trained model in MD simulation with Gromacs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ca1f6",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91559377-60e1-421a-a51e-5e78f0c1b99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threads = 2\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS']=str(threads)\n",
    "import tensorflow as tf\n",
    "\n",
    "# PyTorch favours OMP_NUM_THREADS in environment\n",
    "import torch\n",
    "\n",
    "# Tensorflow needs explicit cofig calls\n",
    "tf.config.threading.set_inter_op_parallelism_threads(threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70ab11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from tensorflow import keras\n",
    "import keras_tuner\n",
    "import asmsa\n",
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba1b8f-d0ff-4264-9712-d06ad9ac964f",
   "metadata": {},
   "source": [
    "## Input files\n",
    "\n",
    "All input files are prepared (up- or downloaded) in [prepare.ipynb](prepare.ipynb). \n",
    "\n",
    "This is for demonstration purpose, in real use the inputs should be placed here, and _conf, traj, topol, index_ variables set to their filenames names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3c43e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input files\n",
    "\n",
    "# input conformation\n",
    "#conf = \"alaninedipeptide_H.pdb\"\n",
    "conf = \"trpcage_correct.pdb\"\n",
    "\n",
    "# input trajectory\n",
    "# atom numbering must be consistent with {conf}\n",
    "\n",
    "#traj = \"alaninedipeptide_reduced.xtc\"\n",
    "traj = \"trpcage_red.xtc\"\n",
    "\n",
    "# input topology\n",
    "# expected to be produced with \n",
    "#    gmx pdb2gmx -f {conf} -p {topol} -n {index} -o {gro}\n",
    "\n",
    "# Gromacs changes atom numbering, the index file must be generated and used as well\n",
    "# gro file is used to generate inverse indexing for plumed.dat\n",
    "\n",
    "#topol = \"topol.top\"\n",
    "topol = \"topol_correct.top\"\n",
    "index = 'index_correct.ndx'\n",
    "gro = 'trpcage_correct.gro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100f507",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96d527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "X_train = tf.data.Dataset.load('datasets/intcoords/train')\n",
    "X_train_np = np.stack(list(X_train))\n",
    "X_train_np = X_train_np[:int(0.5*len(X_train_np))]\n",
    "\n",
    "# load validation dataset\n",
    "X_validate = tf.data.Dataset.load('datasets/intcoords/validate')\n",
    "X_validate_np = np.stack(list(X_validate))\n",
    "\n",
    "X_train_np.shape, X_validate_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a3d4e-86dc-4c14-840d-78a81c1e7d33",
   "metadata": {},
   "source": [
    "## Hyperparameter definition\n",
    "Specify hyperparameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372c4d8-f319-4f28-9290-6d0c633da890",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_hp = {\n",
    "    'activation' : ['relu','gelu','selu'],\n",
    "    'ae_neuron_number_seed' : [32,96,128],\n",
    "    'disc_neuron_number_seed' : [32,96],\n",
    "    'ae_number_of_layers' : [2,5],\n",
    "    'disc_number_of_layers' : [2,5],\n",
    "    'batch_size' : [64,128,256,512],\n",
    "    'optimizer' : ['Adam'],\n",
    "    'learning_rate' : 0.0002,\n",
    "    'ae_loss_fn' : ['MeanSquaredError'],\n",
    "    'disc_loss_fn' : ['BinaryCrossentropy']\n",
    "}\n",
    "\n",
    "tiny_hp = {\n",
    "    'activation' : ['relu'],\n",
    "    'ae_neuron_number_seed' : [32,96],\n",
    "    'disc_neuron_number_seed' : [32,96],\n",
    "    'ae_number_of_layers' : [2,2],\n",
    "    'disc_number_of_layers' : [3,3],\n",
    "    'batch_size' : [64,128,256],\n",
    "    'optimizer' : ['Adam'],\n",
    "    'learning_rate' : 0.0002,\n",
    "    'ae_loss_fn' : ['MeanSquaredError'],\n",
    "    'disc_loss_fn' : ['BinaryCrossentropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3092061-7a90-4a37-b2b4-4606a9fadef1",
   "metadata": {},
   "source": [
    "## Sequential hyperparameter tuning\n",
    "\n",
    "This is robust, it does not require Kubernetes environment for additional job submission. On the other hand, it is slow accordingly.\n",
    "\n",
    "**Skip to the next section if you run the notebook in our recommended setup in Kubernets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2080a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just testing numbers of epochs and hyperparameter setting trials\n",
    "# Don't expect anything meaningful\n",
    "trials=4\n",
    "epochs=100\n",
    "\n",
    "# Set RESULTS_DIR env variable for results of tuning\n",
    "os.environ['RESULTS_DIR'] = datetime.today().strftime(\"%m%d%Y-%H%M%S\")\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    max_trials=trials,\n",
    "    hypermodel=\n",
    "        asmsa.AAEHyperModel(\n",
    "            (X_validate_np.shape[1],),\n",
    "            hp=medium_hp,\n",
    "            prior=tfp.distributions.Normal(loc=1, scale=1)),\n",
    "    objective=keras_tuner.Objective(\"score\", direction=\"min\"),\n",
    "    directory=\"./results\",\n",
    "    project_name=\"Random\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141ae3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner.search(train=X_train_np,validation=X_validate_np,epochs=epochs,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31d980-e476-4956-9151-66d4433c3fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from asmsa.tuning_analyzer import TuningAnalyzer\n",
    "\n",
    "# Create analyzer object that analyses results of tuning\n",
    "# By default it is the latest tuning, but can by choosen with tuning flag,\n",
    "#  e.g TuningAnalyzer(tuning='analysis/05092023-135249')\n",
    "analyzer = TuningAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21acdf17-9123-44ff-8b30-12c3115064ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get sorted hyperparameters by score, by default 10 best HP, for different number:\n",
    "#  analyzer.get_best_hp(num_trials=3)\n",
    "analyzer.get_best_hp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a540e35-2d13-44b2-bc14-86427ded9338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Matplotlib visualization - not recommended way, does not look that good and does not scale \n",
    "#  that well but at least the colors are consistent accross measures. After more work could look better\n",
    "# - By default visualizing best 10 trials\n",
    "# - Can specify only one specific trial... analyzer.visualize_tuning(trial='15d9fa928a7517004bcb28771bb6e5f17ad66dd7013c6aa1572a84773d91393c')\n",
    "# - Can specify number of best trials to be visualized... analyzer.visualize_tuning(num_trials=3)\n",
    "analyzer.visualize_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829d890-6ab3-490a-b6d4-308ad809492a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recommended option via Tensorboard. This function populates TB event\n",
    "#  which can be viewed in native way via Tensorboard. By default chooses\n",
    "#  latest tuning and populates into its directory _TB, e.g: analysis/05092023-135249/_TB\n",
    "# - Can override directory where to populate... analyzer.populate_TB(out_dir='MyTBeventDir')\n",
    "# - Can choose only specific trials via list... analyzer.populate_TB(trials=['15d9fa928a7517004bcb28771bb6e5f17ad66dd7013c6aa1572a84773d91393c']),\n",
    "# - Can select how many best trials to be visualized... analyzer.populate_TB(num_trials=3)\n",
    "analyzer.populate_TB(num_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd9b4a-f5cd-4627-b379-51836ae3f9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7401d7d-35b1-43db-9446-7f1e3714994a",
   "metadata": {},
   "source": [
    "## Parallel hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e992cd-368e-4162-9eeb-8295b9478e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, this is the real stuff\n",
    "# medium settings known to be working for trpcage\n",
    "\n",
    "epochs=15\n",
    "trials=3\n",
    "hp=medium_hp\n",
    "\n",
    "# testing only\n",
    "#epochs=8\n",
    "#trials=6\n",
    "#hp=tiny_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ee1d0-ae41-4e3c-91de-9910eca8b3c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of parallel workers, each runs a single trial at time\n",
    "# balance between resource availability and size of the problem\n",
    "# currently each slave runs on 4 cores and 4 GB RAM (hardcoded in src/asmsa/tunewrapper.py)\n",
    "\n",
    "slaves=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775255db-6e7e-4042-9e29-4a6a38705459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XXX: Kubernetes magic: find out names of container image and volume\n",
    "# check the result, it can go wrong\n",
    "\n",
    "with open('IMAGE') as img:\n",
    "    image=img.read().rstrip()\n",
    "\n",
    "import re\n",
    "mnt=os.popen('mount | grep /home/jovyan').read()\n",
    "pvcid=re.search('pvc-[0-9a-z-]+',mnt).group(0)\n",
    "pvc=os.popen(f'kubectl get pvc | grep {pvcid} | cut -f1 -d\" \"').read().rstrip()\n",
    "\n",
    "print(f\"\"\"\\\n",
    "image: {image}\n",
    "volume: {pvc}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d954868-09bd-427a-ae6a-4334c6ab55d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python wrapper around scripts that prepare and execute parellel Keras Tuner in Kubernetes\n",
    "from asmsa.tunewrapper import TuneWrapper\n",
    "\n",
    "wrapper = TuneWrapper(ds=X_validate_np,hp=hp,output=datetime.today().strftime(\"%m%d%Y-%H%M%S\"),epochs=epochs,trials=trials,pdb=conf,top=topol,xtc=traj,ndx=index, pvc=pvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d894e-b03f-490a-8c8d-cd5310ab2ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Necessary but destructive cleanup before hyperparameter tuning\n",
    "\n",
    "# DON'T RUN THIS CELL BLINDLY\n",
    "# it kills any running processes including the workers, and it purges previous results\n",
    "\n",
    "!kubectl delete job/tuner\n",
    "!kill $(ps ax | grep tuning.py | awk '{print $1}')\n",
    "!rm -rf results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666446de-eb9f-487e-860a-6efadd2ae065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start the master (chief) of tuners in background\n",
    "# the computation takes rather long, this is a more robust approach then keeping it in the notebook\n",
    "\n",
    "wrapper.master_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576a47a-23c6-46b8-b0ba-d184e8598390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# therefore one should check the status ocassionally; it should show a tuning.py process running\n",
    "print(wrapper.master_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5eb38-fb24-4236-8349-7f694bea3f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spawn the requested number of workers as separate Kubernetes job with several pods \n",
    "# they receive work from \n",
    "\n",
    "wrapper.workers_start(num=slaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f752369-13d2-4ed0-be1e-115b446dc35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This status should show {slaves} number of pods, all of them start in Pending state, and follow through ContainerCreating \n",
    "# to Running, and Completed finally\n",
    "\n",
    "# This takes time, minutes to hours depending on size of the model, number of trials, and number of slaves\n",
    "# Run this cell repeatedly, waiting until all the pods are completed\n",
    "\n",
    "wrapper.workers_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec269b7-0096-4861-9c86-3e7f87bce9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same steps for analysis as with serial tuning\n",
    "analyzer = TuningAnalyzer()\n",
    "analyzer.get_best_hp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536d3cf-109b-4719-a88f-42031becd2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can choose output dir for TB event this time\n",
    "out = 'dist_tuning'\n",
    "\n",
    "analyzer.populate_TB(out_dir=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683eb5f-6d7e-4b02-89f5-ea6f7c8b0130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Might need to kill previous tensorboard instance to change logdir\n",
    "!pkill -f 'tensorboard'\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22452d5-a5ac-4ccf-8231-c9b1a2835613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
